{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33713fd6",
   "metadata": {},
   "source": [
    "# Feature Selection in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a05954",
   "metadata": {},
   "source": [
    "- Feature Selection is the process of selecting a subset of relevant features (predictor variables) for use in model construction. \n",
    "- It helps improve model performance, reduces overfitting, and decreases training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ed6bbb",
   "metadata": {},
   "source": [
    "# Why Feature Selection is Important\n",
    "| Benefit               | Description                            |\n",
    "| --------------------- | -------------------------------------- |\n",
    "| üéØ Improves Accuracy  | Removes noise and irrelevant data      |\n",
    "| ‚ö° Reduces Overfitting | Less chance to learn spurious patterns |\n",
    "| üöÄ Speeds up Training | Fewer features = faster computation    |\n",
    "| üìâ Simplifies Models  | Easier to interpret and visualize      |\n",
    "| üíæ Reduces Storage    | Smaller dataset size                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4ba6ea",
   "metadata": {},
   "source": [
    "# üîç Types of Feature Selection Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535295b8",
   "metadata": {},
   "source": [
    "### 1. Filter Methods (Statistical Tests)\n",
    "- These methods use statistical techniques to score the relevance of features with respect to the target.\n",
    "- üìå Common Techniques:\n",
    "\n",
    "\n",
    "| Method                                  | Description                                       | Suitable For                       |\n",
    "| --------------------------------------- | ------------------------------------------------- | ---------------------------------- |\n",
    "| **Variance Threshold**                  | Removes low-variance features                     | All data types                     |\n",
    "| **Correlation Coefficient (Pearson)**   | Removes highly correlated features                | Numeric-Numeric                    |\n",
    "| **Chi-Square Test**                     | Measures dependence between variables             | Categorical input & output         |\n",
    "| **ANOVA F-test**                        | Checks if mean values differ significantly        | Numeric input & categorical output |\n",
    "| **Mutual Information**                  | Measures shared information between variables     | All types                          |\n",
    "| **Information Gain**                    | Reduction in entropy after splitting on a feature | Classification                     |\n",
    "| **Fisher Score**                        | Measures the discriminative power of a feature    | Binary classification              |\n",
    "| **Kendall / Spearman Rank Correlation** | Non-parametric rank-based correlation             | Ordinal/Numeric                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150b018b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Code Using Scikit-learn\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "selector = SelectKBest(score_func=chi2, k=10)\n",
    "X_new = selector.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c8a49",
   "metadata": {},
   "source": [
    "### 2. Wrapper Methods (Model-based Search)\n",
    "- These methods evaluate feature subsets by training a model on them and selecting the best performing combination.\n",
    "- Common Techniques:\n",
    "\n",
    "| Method                                  | Description                                       | Process                |\n",
    "| --------------------------------------- | ------------------------------------------------- | ---------------------- |\n",
    "| **Forward Selection**                   | Start with no features; add features one by one   | Greedy inclusion       |\n",
    "| **Backward Elimination**                | Start with all features; remove least significant | Greedy removal         |\n",
    "| **Stepwise Selection**                  | Combination of forward and backward methods       | Bidirectional search   |\n",
    "| **Recursive Feature Elimination (RFE)** | Recursively remove features based on model weight | Uses model performance |\n",
    "| **Exhaustive Feature Selection**        | Try all combinations of features                  | Very slow but optimal  |\n",
    "| **Sequential Feature Selector (SFS)**   | Sklearn‚Äôs sequential wrapper for forward/backward | Uses cross-validation  |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e9a2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example RFE with Logistic Regression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=5)\n",
    "X_rfe = rfe.fit_transform(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03926bcf",
   "metadata": {},
   "source": [
    "### 3. Embedded Methods (Built into Models)\n",
    "- These methods perform feature selection as part of the model training process.\n",
    "- Examples:\n",
    "\n",
    "| Method                                             | Description                                               | Suitable Models          |\n",
    "| -------------------------------------------------- | --------------------------------------------------------- | ------------------------ |\n",
    "| **Lasso Regression (L1 Regularization)**           | Shrinks some coefficients to zero                         | Linear Models            |\n",
    "| **Ridge Regression (L2 Regularization)**           | Shrinks coefficients but doesn't remove                   | Linear Models            |\n",
    "| **ElasticNet**                                     | Combination of L1 and L2                                  | Linear Models            |\n",
    "| **Tree-based Feature Importance**                  | Extract importance from decision trees                    | RF, XGBoost, etc.        |\n",
    "| **Embedded Feature Selection in LightGBM/XGBoost** | Feature importance via gain, split, cover                 | Gradient Boosting Models |\n",
    "| **Stability Selection**                            | Selects features that appear frequently across bootstraps | Regularized Models       |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ea27ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example (Lasso)\n",
    "from sklearn.linear_model import Lasso\n",
    "model = Lasso(alpha=0.01)\n",
    "model.fit(X, y)\n",
    "importance = model.coef_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
