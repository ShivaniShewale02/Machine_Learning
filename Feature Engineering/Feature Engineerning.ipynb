{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30fb8a40",
   "metadata": {},
   "source": [
    "# Feature Engineering in Machine Learning\n",
    "Feature engineering is one of the most critical steps in a machine learning pipeline. It significantly impacts model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a325a",
   "metadata": {},
   "source": [
    "### What is Feature Engineering?\n",
    "Feature Engineering is the process of creating, transforming, or selecting input features to improve model performance. It involves using domain knowledge and data transformation techniques to help the algorithm better understand the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0493b7c5",
   "metadata": {},
   "source": [
    "##### Why Feature Engineering is Important?\n",
    "- Boosts model accuracy\n",
    "\n",
    "- Reduces overfitting/underfitting\n",
    "\n",
    "- Simplifies complex data\n",
    "\n",
    "- Helps models interpret hidden patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3d327",
   "metadata": {},
   "source": [
    "### Core Steps in Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7c4a4b",
   "metadata": {},
   "source": [
    "##### 1. Feature Creation (Deriving New Features)\n",
    "Creating new features from existing ones to provide more useful signals.\n",
    "\n",
    "ðŸ“Œ Examples:\n",
    "- BMI = Weight / Height^2\n",
    "- Age from DOB\n",
    "- Total_Amount = Quantity * Unit_Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1435e",
   "metadata": {},
   "source": [
    "##### 2. Feature Transformation\n",
    "Transforming the data to meet the assumptions of algorithms or to normalize the range.\n",
    "\n",
    "Common Transformation:\n",
    "\n",
    "| Transformation      | Description                  | Example                 |\n",
    "| ------------------- | ---------------------------- | ----------------------- |\n",
    "| **Log Transform**   | Handles skewed data          | `log(x + 1)`            |\n",
    "| **Square Root**     | Normalizes wide-ranging data | `sqrt(x)`               |\n",
    "| **Standardization** | Mean = 0, Std = 1            | `(x - mean)/std`        |\n",
    "| **Normalization**   | Scale \\[0,1]                 | `(x - min)/(max - min)` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208f68cd",
   "metadata": {},
   "source": [
    "##### 3. Encoding Categorical Variables\n",
    "Categorical features need to be converted to numeric format.\n",
    "\n",
    "Encoding Techniques:\n",
    "  \n",
    "| Technique            | Use Case                     | Example                           |\n",
    "| -------------------- | ---------------------------- | --------------------------------- |\n",
    "| **Label Encoding**   | Ordinal Data                 | `{'Low':0, 'Medium':1, 'High':2}` |\n",
    "| **One-Hot Encoding** | Nominal Data                 | `pd.get_dummies()`                |\n",
    "| **Binary Encoding**  | High-cardinality categorical | Reduces dimensionality            |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d7469",
   "metadata": {},
   "source": [
    "##### 4. Handling Missing Data\n",
    "Missing values can mislead the model.\n",
    "\n",
    "ðŸ“Œ Strategies:\n",
    "- Mean/Median/Mode Imputation\n",
    "- Forward Fill / Backward Fill\n",
    "- Using a placeholder (e.g., -999)\n",
    "- ML-based Imputation (KNN, MICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1650faa9",
   "metadata": {},
   "source": [
    "##### 5. Handling Outliers\n",
    "Outliers can heavily affect model performance.\n",
    "ðŸ“Œ Techniques:\n",
    "- Z-score: If |z| > 3, it's likely an outlier\n",
    "- IQR Method: Outliers = x < Q1 - 1.5*IQR or x > Q3 + 1.5*IQR\n",
    "- Capping: Replace with max/min threshold values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba2962a",
   "metadata": {},
   "source": [
    "##### 6. Feature Selection\n",
    "Removing irrelevant or redundant features improves speed and avoids overfitting.\n",
    "\n",
    "ðŸ“Œ Methods:\n",
    "- Filter Methods: Correlation, Chi-Square\n",
    "- Wrapper Methods: Recursive Feature Elimination (RFE)\n",
    "- Embedded Methods: Lasso (L1), Tree-based models (e.g., Feature Importance in RandomForest)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e98a105",
   "metadata": {},
   "source": [
    "##### 7. Discretization (Binning)\n",
    "Converts continuous data into categorical bins.\n",
    "\n",
    "ðŸ“Œ Example:\n",
    "Convert age into:\n",
    "- 0â€“18: Child\n",
    "- 19â€“35: Young Adult\n",
    "- 36â€“60: Adult\n",
    "- 60+: Senior\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c04a447",
   "metadata": {},
   "source": [
    "##### 8. Polynomial Features / Interaction Features\n",
    "Creating interaction terms or polynomial powers to capture complex relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6230ff",
   "metadata": {},
   "source": [
    "##### 9. Datetime Feature Extraction\n",
    "\n",
    "Extract meaningful components from date/time columns:\n",
    "- Year, Month, Day\n",
    "- Day of week\n",
    "- Is weekend\n",
    "- Time delta between dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06cd6a3a",
   "metadata": {},
   "source": [
    "##### 10. Text Feature Engineering\n",
    "For NLP tasks:\n",
    "- Bag of Words\n",
    "- TF-IDF\n",
    "- Word Embeddings\n",
    "- Keyword extraction\n",
    "- Sentiment scores"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
